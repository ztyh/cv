\documentclass[utf8,letterpaper,oneside]{article}
% \documentclass[letterpaper,oneside]{ctexart}
% \usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{graphicx}
%\usepackage{xeCJK}
%\usepackage{fontspec}
\usepackage{xcolor}
\usepackage{color,soul}
\usepackage{textcomp}
%\setmainfont{kaiu.ttf}
\graphicspath{ {images/}}
\usepackage[left=0.5in, right=0.5in, bottom=0.5in, top=0.5in]{geometry}
\newcommand*{\Skype}{\href{skype:john.smith?add}{john.smith}}
\newcommand{\Absender}[1][\normalsize]{\Skype}
\usepackage[symbol]{footmisc}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\pagenumbering{gobble}
\setlength\parindent{0pt}
\begin{document}
Dear Mr. or Ms.
\vskip 1cm
I am writing to you in hope of being able to work with you. I think my experiences align well with the role at your company.
\vskip 1cm
\subsubsection*{Elaboration on Coursework}
This has involved many programming projects. I have also attended numerous seminars on statistics. The statistical instruction company Aidemy provides me with free access to their charged courses as well as their private courses. There, I have learned things such as reinforcement learning, image processing, data scraping, and natural language processing.
\subsubsection*{Elaboration on Teaching}
I have taught for four semesters. STAT 463 (a Junior and Senior level time series course), STAT 415 (Junior and Senior level statistical theory course), STAT 401 (twice, an easier version of STAT 415). For the last two semesters of teaching I have provided materials on my website. You may wish to take a look. You may wish to also take a look at my blog, the link of which is also provided on my website. On my blog, you may also see my \texttt{github} account. Although not all work is made public. I have also graded two Ph.D. level courses, STAT 513 and STAT 514 (theory of statistics I and II). These two cover the most important theoretical knowledge needed in the Ph.D. study of statistics.
\subsubsection*{Elaboration on Projects and Practical Experiences}
Because my undergraduate department had high interest in time series analysis, I had studied literatures on how to perform nonlinear modelling of quantiles in time series data in the early stages of graduate school. The core of the idea was to do separate regressions on a shifting window. For my undergraduate graduation project, I studied the repeating patterns of prices for stocks. Each stock has a coefficient for cosine functions of differnt frequencies. The idea was that the cosine function that only completes a half cycle corresponds to the trend. So we want to find a linear combination of the stocks that minimizes the coefficient on the trend while minimizing the coefficients of all other cosine components which corresponds to variance. This final part was done with linear programming. During my undergraduate study, I extensively coded \texttt{c} implementations of numerical calculations of derivatives. Substitute charge method finds a solution that satisfies certain conditions by using basis functions that satisfy them. It is so called because for certain problems, these basis functions are exactly the ones that appear when you calculate the force by a charge in physics. Finite element method is another, more common way of calculating derivative prices. Also used in engineering, it approximates a continuous phenomenon by a discrete grid.
\subsubsection*{Elaboration on Research}
Currently I do MAVE (minimum average variance estimation) with neural networks. MAVE finds the dimension reducing matrix $W$ that minimizes the error in performing piecewise linear regression with $WX$. Neural networks can do nonlinear regression intrinsically. So we do not need to perform piecewise regression. In fact the matrix factor in the first affine transform in the neural network is exactly the $W$ we are looking for. We use this as well as variants of it to perform dimension reduction. Another project that I am currently doing is finding the standard error of a neural network estimator. This involves taking the second derivatives of the model. Due to the product rule, this derivation becomes much more complicated than the first derivative. Nonetheless it is something that can be done. Furthermore, I have found ways to circumvent taking the second derivative. Previously I had done support vector machine classifiers for multiple functional data using RKHS methods. This was unique in that it can handle multiple functions. It also gave superior results on real data on finding unhealthy patients from data on brain waves in multiple regions of the brain. I have also looked at combining global sufficient dimension reduction methods such as contour regression and directional regression with local dimension reduction methods such as MAVE. Other topics that I am doing include graph embedding, especially on non-traditional spaces. Graphs where vertices are functions can be embedded into traditional as well as non-traditional spaces. This has the benefit of not having to encode the edges, which increases in the order of $n \choose 2$. Embeddings has points of order $n$ instead. The similarity of pairs of points can be checked by the distance between them in the embedded space.
\vskip 1cm
Thank you for taking the time to consider me. I hope we may keep in touch.
\vskip 1cm
Sincerely,\\
Zheye
\end{document}
